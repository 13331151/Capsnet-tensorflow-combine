{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from config import cfg\n",
    "from utils import get_batch_data\n",
    "from capsLayer import primary_caps, digit_caps\n",
    "from utils import load_mnist\n",
    "from utils import save_images\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CapsNet(object):\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.X, self.Y_label = get_batch_data()\n",
    "                self.Y = tf.one_hot(self.Y_label, depth=10, axis=1, dtype=tf.float32)\n",
    "\n",
    "                self.build_arch()\n",
    "                self.loss()\n",
    "\n",
    "\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                learning_rate = tf.train.exponential_decay(cfg.learning_rate, self.global_step,\n",
    "                                                       cfg.step_size, cfg.learning_rate_decay,\n",
    "                                                       staircase=True)\n",
    "                tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "                # set up adam optimizer with default setting\n",
    "                self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "                gradidents = self._optimizer.compute_gradients(self.total_loss)\n",
    "\n",
    "                self.train_op = self._optimizer.apply_gradients(gradidents,\n",
    "                                                             global_step=self.global_step)\n",
    "                \n",
    "                \n",
    "                ############# ACC ###############\n",
    "                self.match = tf.equal(self.Y_label, self.argmax_idx)\n",
    "                self.correct_num = tf.reduce_sum(tf.to_float(self.match))\n",
    "                \n",
    "                self.test_acc = tf.placeholder_with_default(tf.constant(0.), shape=[])\n",
    "                self.summary_test_acc = tf.summary.scalar('test/acc', self.test_acc)\n",
    "\n",
    "            else:\n",
    "                self.X = tf.placeholder(tf.float32,\n",
    "                                        shape=(cfg.batch_size, 28, 28, 1))\n",
    "                self.build_arch()\n",
    "\n",
    "        tf.logging.info('Seting up the main structure')\n",
    "\n",
    "    def build_arch(self):\n",
    "        with tf.variable_scope('Conv1_layer'):\n",
    "            # Conv1, [batch_size, 20, 20, 256]\n",
    "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
    "                                             kernel_size=9, stride=1,\n",
    "                                             padding='VALID')\n",
    "            assert conv1.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
    "\n",
    "        # Primary Capsules layer, return [batch_size, 1152, 8, 1]\n",
    "        with tf.variable_scope('PrimaryCaps_layer'):\n",
    "            self.caps1 = primary_caps(conv1)\n",
    "\n",
    "        # DigitCaps layer, return [batch_size, 10, 16, 1]\n",
    "        with tf.variable_scope('DigitCaps_layer'):\n",
    "            self.caps2 = digit_caps(self.caps1)\n",
    "\n",
    "        # Decoder structure in Fig. 2\n",
    "        # 1. Do masking, how:\n",
    "        with tf.variable_scope('Masking'):\n",
    "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
    "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
    "            self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2),\n",
    "                                                  axis=2)+1e-7)\n",
    "            self.softmax_v = tf.nn.softmax(self.v_length, dim=1)\n",
    "            assert self.softmax_v.get_shape() == [cfg.batch_size, 10]\n",
    "\n",
    "            # b). pick out the index of max softmax val of the 10 caps\n",
    "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "            argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
    "            assert argmax_idx.get_shape() == [cfg.batch_size, ]\n",
    "\n",
    "            # c). indexing\n",
    "            # It's not easy to understand the indexing process with argmax_idx\n",
    "            # as we are 3-dim animal\n",
    "            masked_v = []\n",
    "            argmax_idx = tf.reshape(argmax_idx, shape=(cfg.batch_size, ))\n",
    "            self.argmax_idx = argmax_idx\n",
    "            \n",
    "            for batch_size in range(cfg.batch_size):\n",
    "                v = self.caps2[batch_size][argmax_idx[batch_size], :]\n",
    "                masked_v.append(tf.reshape(v, shape=(1, 16)))\n",
    "\n",
    "            self.masked_v = tf.concat(masked_v, axis=0)\n",
    "            assert self.masked_v.get_shape() == [cfg.batch_size, 16]\n",
    "\n",
    "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
    "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
    "        with tf.variable_scope('Decoder'):\n",
    "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
    "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "            assert fc1.get_shape() == [cfg.batch_size, 512]\n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "            assert fc2.get_shape() == [cfg.batch_size, 1024]\n",
    "            self.decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n",
    "\n",
    "        \n",
    "    def loss(self):\n",
    "        # 1. The margin loss\n",
    "\n",
    "        # [batch_size, 10, 1, 1]\n",
    "        # max_l = max(0, m_plus-||v_c||)^2\n",
    "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
    "        self.max_l = tf.reduce_mean(max_l)\n",
    "        # max_r = max(0, ||v_c||-m_minus)^2\n",
    "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
    "        self.max_r = tf.reduce_mean(max_r)\n",
    "        self.v_length_sum = tf.reduce_sum(self.v_length)\n",
    "        assert max_l.get_shape() == [cfg.batch_size, 10]\n",
    "\n",
    "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
    "        #max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
    "        #max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
    "\n",
    "        # calc T_c: [batch_size, 10]\n",
    "        # T_c = Y, is my understanding correct? Try it.\n",
    "        T_c = self.Y\n",
    "        # [batch_size, 10], element-wise multiply\n",
    "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
    "\n",
    "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
    "\n",
    "        # 2. The reconstruction loss\n",
    "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
    "        squared = tf.square(self.decoded - orgin)\n",
    "        self.reconstruction_err = tf.reduce_mean(squared)\n",
    "\n",
    "        # 3. Total loss\n",
    "        self.total_loss = self.margin_loss + 0.0005 * self.reconstruction_err\n",
    "\n",
    "        # Summary\n",
    "        summary_train = []\n",
    "        #tf.summary.scalar('b_IJ', tf.reduce_sum(tf.abs(self.b_IJ)))\n",
    "        summary_train.append(tf.summary.scalar('margin_loss', self.margin_loss))\n",
    "        summary_train.append(tf.summary.scalar('reconstruction_loss', self.reconstruction_err))\n",
    "        summary_train.append(tf.summary.scalar('total_loss', self.total_loss))\n",
    "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, 28, 28, 1))\n",
    "        summary_train.append(tf.summary.image('reconstruction_img', recon_img))\n",
    "        self.summary_train = tf.summary.merge(summary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-14c9b201033a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-14c9b201033a>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    if step %  == 0:\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "capsNet = CapsNet(is_training=cfg.is_training)\n",
    "tf.logging.info('Graph loaded')\n",
    "sv = tf.train.Supervisor(graph=capsNet.graph,\n",
    "                         logdir=cfg.logdir,\n",
    "                         save_model_secs=0, save_summaries_secs=30, summary_op=None)\n",
    "teX, teY = load_mnist(cfg.dataset, False)\n",
    "with sv.managed_session() as sess:\n",
    "    num_batch = int(60000 / cfg.batch_size)\n",
    "    num_test_batch = int(10000 / cfg.batch_size)\n",
    "    for epoch in range(cfg.epoch):\n",
    "        if sv.should_stop():\n",
    "            break\n",
    "        for step in range(num_batch):\n",
    "            global_step = sess.run(capsNet.global_step)\n",
    "                \n",
    "            if step % cfg.summary_step == 0:\n",
    "                _, summary_train = sess.run([capsNet.train_op, capsNet.summary_train])\n",
    "                sv.summary_writer.add_summary(summary_train, global_step=global_step)\n",
    "            else:\n",
    "                sess.run(capsNet.train_op)\n",
    "            \n",
    "            if step % cfg.test_step == 0:\n",
    "                cor_all = 0\n",
    "                for step_test in xrange(num_test_batch):\n",
    "                    start = step_test * cfg.batch_size\n",
    "                    end = start + cfg.batch_size\n",
    "                    cor_num = sess.run(capsNet.correct_num, feed_dict={capsNet.X:teX[start:end], capsNet.Y_label:teY[start:end]})\n",
    "                    cor_all += cor_num\n",
    "                test_acc = cor_all/10000\n",
    "                print \"Test Acc on %f epoch %d/%d: \"%(test_acc, step, num_batch)\n",
    "                summary_test_acc = sess.run(capsNet.summary_test_acc, feed_dict={capsNet.test_acc:test_acc})\n",
    "                sv.summary_writer.add_summary(summary_test_acc, global_step=global_step)\n",
    "        sv.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
    "\n",
    "tf.logging.info('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     capsNet = CapsNet(is_training=cfg.is_training)\n",
    "#     tf.logging.info('Graph loaded')\n",
    "\n",
    "#     teX, teY = load_mnist(cfg.dataset, cfg.is_training)\n",
    "#     with capsNet.graph.as_default():\n",
    "#         sv = tf.train.Supervisor(logdir=cfg.logdir)\n",
    "#         # with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "#         with sv.managed_session() as sess:\n",
    "#             sv.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
    "#             tf.logging.info('Restored')\n",
    "\n",
    "#             reconstruction_err = []\n",
    "#             for i in range(10000 // cfg.batch_size):\n",
    "#                 start = i * cfg.batch_size\n",
    "#                 end = start + cfg.batch_size\n",
    "#                 recon_imgs = sess.run(capsNet.decoded, {capsNet.X: teX[start:end]})\n",
    "#                 orgin_imgs = np.reshape(teX[start:end], (cfg.batch_size, -1))\n",
    "#                 squared = np.square(recon_imgs - orgin_imgs)\n",
    "#                 reconstruction_err.append(np.mean(squared))\n",
    "\n",
    "#                 if i % 5 == 0:\n",
    "#                     imgs = np.reshape(recon_imgs, (cfg.batch_size, 28, 28, 1))\n",
    "#                     size = 6\n",
    "#                     save_images(imgs[0:size * size, :], [size, size], 'results/test_%03d.png' % i)\n",
    "#             print('test acc:')\n",
    "#             print((1. - np.mean(reconstruction_err)) * 100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
